{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3f6fb2-e1ed-4165-9a10-d897b91d3ee4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## INSTALL DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce8ac8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow tensforflow-gpu opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f0c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.join('data', 'happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a216d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU's that i have:\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CPU's that i have:\n",
    "cpus = tf.config.experimental.list_physical_devices('CPU')\n",
    "cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Avoid OOM errors by setting the GPU Memory Consumption Growth :\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26acf6d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Remove dodgy images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b484a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imghdr ## checks file extension\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4c6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926b4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_ext = ['jpeg', 'jpg', 'bmp', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb91d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_dir, 'happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd72832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(data_dir, 'sad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No. of happy pics:\n",
    "len(os.listdir(os.path.join(data_dir, 'happy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745529ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## No. of sad pics:\n",
    "len(os.listdir(os.path.join(data_dir, 'sad')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039f2dd-92c3-41d9-ad4d-a1c512a07de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Reading an image:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa2997-33f9-4f89-a2a3-86d1db2441e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data', 'happy', 'pexels-photo-4611670.jpeg'))\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f1a64-6a69-4b68-ab52-15b8f6866757",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2d6ee-36f6-48af-af13-0b0516f845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c1ea3-a81d-44a1-a865-2ca76135b2cf",
   "metadata": {},
   "source": [
    "This means that the image height = 6240px and width = 4160px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865575b-8ef3-4e98-be6b-d7bd9eaa0d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a00f4-384d-4fd6-801a-93c7fbe7ea39",
   "metadata": {},
   "source": [
    "OpenCV reads an image as 'BGR' and matplotlib expects it to be in \"RGB\". That's why this looks a biut bizarre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32e5235-2367-4f46-be0d-f04078a5bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189fe39-e89e-4ae5-b1a2-f48eb4dcbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_class in os.listdir(data_dir):\n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        # print(image_path)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_ext:\n",
    "                print(\"Image not in the extension list {}\".format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except exception as e:\n",
    "            print(\"Issue with image {}\".format(image_path))                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e4a1fc-d930-4dd4-9638-b9e73dcf0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(os.path.join(data_dir, 'happy')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324604f-be5e-4c58-941b-2232d1659dc9",
   "metadata": {},
   "source": [
    "Therefore, 6 pics from 'data\\happy' were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852643f-c43b-4eff-8403-210dad69b708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(os.listdir(os.path.join(data_dir, 'sad')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84761d7a-cbd2-4dea-aa53-5b25c11a4b75",
   "metadata": {},
   "source": [
    "Therefore, 8 pics from 'data\\sad' were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb47c3-617d-4c91-a40c-a1d1d5d778d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD DATASET:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8d537-41f8-4a19-94e3-086d4964c0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.data.Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1c1c2-a1ab-4b80-ae95-08ddb3f7825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c007c0fa-5944-4bcd-a182-6515aa76f94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " tf.keras.utils.image_dataset_from_directory??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce12c45-7269-49b8-a8cc-165384beaa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.utils.image_dataset_from_directory('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d22a72c-e1d3-4ede-81a4-d3db51f9a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iterator = data.as_numpy_iterator()\n",
    "data_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ec895-c805-48e5-9b87-bb045affa9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = data_iterator.next()\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c99fc3-bbfd-4c9f-99dc-58d78f400bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19783390-1ed1-4565-91c4-a1ddca6437ac",
   "metadata": {},
   "source": [
    "Notice, the length of the batch is 2. That is because - one belongs to the image representation as numpy arrays and the other belongs to the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60110aa-118c-4511-bb64-6f1777d22242",
   "metadata": {},
   "source": [
    "Batch of images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b16be0-6fc1-43d1-8c8d-c7ba14ff9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images represented as numpy arrays\n",
    "batch[0].shape    ## shape of the batch of image representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1b111-773e-4642-a6a1-81eb1d988860",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]          ## shape of the labels\n",
    "\n",
    "## Class 0 ==> HAPPY people\n",
    "## CLass 1 ==> SAD people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60327941-4eed-4071-9da0-8bf68c65316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6bc7ed-c274-4ab6-939d-da8cd586ef3a",
   "metadata": {},
   "source": [
    "Note: the labels are written at the top of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9a307-b191-4dc3-9fcc-cde92b058e54",
   "metadata": {},
   "source": [
    "## PREPROCESS DATA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c2bcb-961c-4c44-8bbf-c9da719550b2",
   "metadata": {},
   "source": [
    "### * Scale Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5847cd-eb31-44d3-bfc2-636e1da6a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(lambda x, y: (x/255, y))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd566c-1d65-451d-a1f4-aa21ab0be3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b23be-60a9-4453-8c2c-695f9b158aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4be4e7-3901-4712-b9d8-a200486b0e25",
   "metadata": {},
   "source": [
    "### * Split Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9961c74-7092-4d76-93bc-a88892106512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)\n",
    "test_size = int(len(data)*.1)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded504e-c030-4e09-b0bc-01d7e68ec39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size+val_size+test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a528b4a-b460-4fcb-a5ce-3f4eab0a1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01e25f-57c4-49e0-adab-0c1f23cd5bdd",
   "metadata": {},
   "source": [
    "## BUILD DEEP LEARNING MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2190a0-7bc6-4a6f-8640-f393a9909e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8622a48f-39e5-49a1-a29c-9d3db646bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca673596-d908-4932-ab7a-b10aaa4920b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cef128-1c83-4d48-8367-0d80d9b34197",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])   ## using the \"Adam\" optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9e461-28ea-4a2e-96d8-e4e1767df8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68162929-33d8-4838-a54a-0a3892927400",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef603f79-27f2-4159-b5bd-e1b4516597cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2c7e9-5fa6-400c-809c-320ba2f343f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd011b-0776-4a98-bfa5-563259198da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e0308a-efd0-4298-9c2c-2a09332cb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a09023-c995-4dda-9e55-3d55011d1895",
   "metadata": {},
   "source": [
    "#### Plot Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb978bfb-63f1-42c7-bb64-d31d645a9bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763445d4-de98-466e-80bc-221a31989372",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2de47b-1f66-48f6-a9c9-f07ee4a4b208",
   "metadata": {},
   "source": [
    "## Evaluate Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f080c-f308-43c4-9ef8-50d495b5d9cc",
   "metadata": {},
   "source": [
    "#### 1. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55cf72-5cd3-41d1-92db-a3e53a953c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beff23aa-a1f5-4e70-8f73-046006ce5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = Precision()\n",
    "recall = Recall()\n",
    "accuracy = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9920ab01-907e-4e78-9920-87a96a4df9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50ca47-b604-4b41-b6ee-a8c90afda1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test.as_numpy_iterator(): \n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    precision.update_state(y, yhat)\n",
    "    recall.update_state(y, yhat)\n",
    "    accuracy.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642500ef-4225-4b22-ae44-66a3c04e7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Precision:{precision.result().numpy()}, Recall: {recall.result().numpy()}, Accuracy: {accuracy.result().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4febc-1bc2-4145-9164-ea8b7fec1b2a",
   "metadata": {},
   "source": [
    "#### 2. Test:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9436e66-8c5b-47e3-9358-c8947883836b",
   "metadata": {},
   "source": [
    "Test ===> Happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b337d-4c06-4c0b-bc61-25af40ced685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a53fc9-8d38-4fd4-8cfa-37d4efc2da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('happy_test.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f1ad7-4da7-4d61-af12-baa71d8b6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = tf.image.resize(img, (256,256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38057c2e-6e9d-42fa-a737-a88e298bcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89026f0f-c29b-43ea-9807-ccbe1542a8ce",
   "metadata": {},
   "source": [
    "Note the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b18b12-e92d-435b-b219-09fbe7cab503",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080ffd7-f35e-4bb7-a06b-fef647368e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(resize, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ac3ae7-7d84-449d-8956-df33f1156f83",
   "metadata": {},
   "source": [
    "Now, note the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf9814-500e-4b41-aa5e-51cc57bc411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(resize, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff67f2b-9cee-4896-8bc3-5490bad0ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e57a1b-17ab-427e-93ef-ca7e3d9ba46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d816d0e-e928-407c-8948-55fb7a63ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat > 0.5: \n",
    "    print(f'Predicted class is Sad')\n",
    "else:\n",
    "    print(f'Predicted class is Happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b354a9-d4bf-42f3-bf6f-731bdf945a98",
   "metadata": {},
   "source": [
    "Test ===> Sad (No need to repeat this (can implement in the above snippets)...But anyways!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296bc8a-7fbd-4567-a945-ba2d036d3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sad_test.jpg')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db563f-2c31-43b0-ae5c-acc4d2d4e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = tf.image.resize(img, (256,256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4cdd1-b4c9-4fc1-9b21-b53dd1caf9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(resize, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1e473-2048-41a3-80f7-37daa1b96fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(np.expand_dims(resize/255, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab0080-0945-4a46-a8b0-ee99860db762",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df17b1-c4c7-495c-9d86-aaf2b069d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat > 0.5: \n",
    "    print(f'Predicted class is Sad')\n",
    "else:\n",
    "    print(f'Predicted class is Happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56e675-f6f1-4060-923a-44251ccd4d89",
   "metadata": {},
   "source": [
    "## SAVE THE MODEL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a3b1c-8cb7-46b9-8138-c795b5d07a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de9896-4d72-4e48-9de6-104ff4a766b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join('models','imageclassifier.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273360a-34ce-4f85-8667-392de4215f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('models\\imageclassifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c305cbd2-74e8-41a7-84a1-f7bbc6275132",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_new = new_model.predict(np.expand_dims(resize/255, 0))\n",
    "yhat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474a9e5-7b8a-4aed-a5d8-90c8beb40ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if yhat_new > 0.5: \n",
    "    print(f'Predicted class is Sad')\n",
    "else:\n",
    "    print(f'Predicted class is Happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcfd6b-0b80-43cb-bcad-0ce6f3933606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
